{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f525524",
   "metadata": {},
   "source": [
    "Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e353046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas scikit-learn sentence-transformers pyvis networkx xlsxwriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405bd5e6",
   "metadata": {},
   "source": [
    "sentence-transformers package needs torch, tensorflow or jax available as backend\n",
    "I used torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03c9f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b10d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3809bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ISSUES_FILE = './all_issues.csv'\n",
    "issue_data = pd.read_csv(ISSUES_FILE)\n",
    "issue_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1d4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_data[\"name\"] = issue_data[\"ID\"] + \" - \" + issue_data[\"WG\"]\n",
    "issue_data[\"text\"] = issue_data[\"Title\"] + \" - \" + issue_data[\"Description\"]\n",
    "issue_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1a73f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_data_all = issue_data.copy()\n",
    "issue_data.drop_duplicates(subset=['ID', 'WG'], inplace=True, ignore_index=True)\n",
    "issue_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63397821",
   "metadata": {},
   "source": [
    "Use pretrained language model to compute semantic textual similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4b3ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L12-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ebf1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_data[\"Embedding\"] = issue_data[\"text\"].apply(lambda x: model.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e832f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity(issue_data[\"Embedding\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f24884",
   "metadata": {},
   "source": [
    "Add edges to graph.\n",
    "An edge existst between (i) and (j) iff issue represented by (j) is most similar to (i)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c34748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "components_graph = nx.DiGraph()\n",
    "\n",
    "components_graph.add_nodes_from([\n",
    "    (tup.Index, {\n",
    "        'title': tup.text,\n",
    "        'label': tup.name,\n",
    "        'group': tup.WG,\n",
    "        'size': 25\n",
    "    })\n",
    "    for tup in issue_data.itertuples()\n",
    "    \n",
    "])\n",
    "\n",
    "for src in range(len(issue_data.index)):\n",
    "    highest_similarity = np.argsort(-similarity[src,])[1:2] # self has highest similarity\n",
    "    for dest in highest_similarity:\n",
    "        weight_val = float(similarity[src,dest])\n",
    "        components_graph.add_edge(src, int(dest), weight=weight_val, value=weight_val, arrowStrikethrough=False)\n",
    "\n",
    "rankings = nx.pagerank(nx.MultiGraph(components_graph), weight='weight')\n",
    "_min = min(rankings.values())\n",
    "_max = max(rankings.values())\n",
    "\n",
    "for node_id, rank in rankings.items():\n",
    "    components_graph.nodes[node_id]['importance'] = 15 + (rank - _min) / (_max - _min) * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6448a56b",
   "metadata": {},
   "source": [
    "Visualize the resulting graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4838900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis import network as net\n",
    "nt = net.Network(notebook=True, height='900px', width='100%', directed=True)\n",
    "nt.barnes_hut(spring_length=100, spring_strength=0.1, central_gravity=8, overlap=1)\n",
    "nt.from_nx(components_graph)\n",
    "nt.show('test.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2267497e",
   "metadata": {},
   "source": [
    "Identify weakly connected components (i.e. clusters) in the graph and save them in a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e549b347",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_df = pd.concat([\n",
    "    pd.DataFrame({\n",
    "        'name': [components_graph.nodes[node_id]['label'] for node_id in comp],\n",
    "        'importance': [components_graph.nodes[node_id]['importance'] for node_id in comp],\n",
    "        'component': index\n",
    "    })\n",
    "    for index, comp in enumerate(sorted(nx.weakly_connected_components(components_graph), key=len, reverse=True), start=1)\n",
    "])\n",
    "components_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db5a8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_data_final = issue_data_all.merge(components_df)\n",
    "issue_data_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed29c10",
   "metadata": {},
   "source": [
    "Export the components to excel, for better sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d91b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('identified_clusters.xlsx') as writer:\n",
    "    for component_number in sorted(issue_data_final['component'].unique()):\n",
    "        component_data = (issue_data_final[issue_data_final['component'] == component_number]\n",
    "                          .sort_values(['component', 'importance'], ascending=[True, False])\n",
    "                          .drop(columns=['name', 'text', 'importance', 'component'])\n",
    "                         )\n",
    "        \n",
    "        component_data.to_excel(writer, index=None, sheet_name=f\"Cluster {component_number}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3e5654",
   "metadata": {},
   "source": [
    "*Additional:* query the dataset for the most similar issue to any given issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626753a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "queryId = \"E1 - technical\"\n",
    "top_K = 5\n",
    "\n",
    "index_row = issue_data[issue_data['name'] == queryId].index.tolist()[0]\n",
    "similar_indices = np.argsort(-similarity[index_row,])[1:top_K+1]\n",
    "\n",
    "issue_data.iloc[similar_indices].itertuples()\n",
    "print(f\"Query: {issue_data.loc[index_row].text}\")\n",
    "print(\"-----\")\n",
    "print(\"Most similar issues:\")\n",
    "for row in issue_data.iloc[similar_indices].itertuples():\n",
    "        print(f\"{row.name}: {row.text}\")\n",
    "        print(f\"(Similarity: {similarity[index_row, row.Index]:.4f})\")\n",
    "        print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
